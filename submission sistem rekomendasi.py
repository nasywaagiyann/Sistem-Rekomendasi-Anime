# -*- coding: utf-8 -*-
"""Copy of FIX REKOM3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EpyY6lhJmbnOM7n3rvKvWLbaeh2DpIiW

**Nasywa Agiyan Nisa**

**MC-28**

**Proyek Sistem Rekomendasi**

## **Overview**
Dalam era digital yang dipenuhi oleh berbagai pilihan hiburan, menemukan anime yang sesuai dengan preferensi individu bisa menjadi tantangan. Oleh karena itu, proyek ini bertujuan untuk membangun sistem rekomendasi anime yang cerdas dengan memanfaatkan pendekatan *Content-Based Filtering* dan *Collaborative Filtering*. Dengan menggabungkan kedua metode ini, sistem dapat memberikan rekomendasi yang lebih akurat dan personal, baik berdasarkan kesamaan konten maupun pola kesukaan pengguna lain yang serupa.

## **1. Data Collection**

Tujuan dari tahap data collection dalam proyek sistem rekomendasi anime adalah untuk mengumpulkan data mentah yang akan menjadi dasar dari seluruh proses analisis dan pembangunan model. Tanpa data yang relevan dan representatif, model tidak akan mampu belajar pola preferensi pengguna atau memahami karakteristik setiap anime.

#### Import Library yang dibutuhkan
"""

# Impor Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import os
import shutil
import zipfile

!pip install -q kaggle

"""Unggah file kaggle.json untuk dapat terhubungan dengan kaggle dataset."""

from google.colab import files
files.upload()

# Membuat direktori .kaggle jika belum ada
os.makedirs("/root/.kaggle", exist_ok=True)

# Pindahkan file ke folder .kaggle
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")

# Ubah permission agar tidak terbaca publik
os.chmod("/root/.kaggle/kaggle.json", 600)

"""#### Mengunduh dataset dari Kaggle dengan mengunakan Kaggle API."""

!kaggle datasets download -d hernan4444/animeplanet-recommendation-database-2020

"""#### Ekstrak zip file"""

with zipfile.ZipFile("animeplanet-recommendation-database-2020.zip", "r") as zip_ref:
    zip_ref.extractall("animeplanet_data")

# Lihat isinya
!ls animeplanet_data

"""#### Membaca berbagai dataset dari Anime-Planet Recommendation Database 2020"""

anime_df = pd.read_csv("animeplanet_data/anime.csv")                  # Data utama tentang anime
animelist_df = pd.read_csv("animeplanet_data/animelist.csv")          # Data daftar anime pengguna
recommendations_df = pd.read_csv("animeplanet_data/anime_recommendations.csv")  # Data rekomendasi antar anime
rating_df = pd.read_csv("animeplanet_data/rating_complete.csv")       # Data rating lengkap dari pengguna
status_df = pd.read_csv("animeplanet_data/watching_status.csv")       # Data status menonton pengguna

"""## **2. Data Understanding**

Data Understanding itu tahap penting dalam proses analisis data atau machine learning. Fungsinya adalah untuk mengenal dan memahami data sebelum mulai diproses lebih lanjut.

#### Memuat dan Memeriksa Dimensi Dataset
"""

# Membuat dictionary yang memetakan nama file dataset ke DataFrame yang sesuai
datasets = {
    "anime.csv": anime_df,
    "animelist.csv": animelist_df,
    "anime_recommendations.csv": recommendations_df,
    "rating_complete.csv": rating_df,
    "watching_status.csv": status_df
}

# Melakukan iterasi untuk setiap item dalam dictionary datasets
for name, df in datasets.items():
    # Menampilkan nama dataset dan dimensinya (jumlah baris dan kolom)
    print(f"{name}: {df.shape}")

    # Menampilkan daftar nama kolom dalam dataset
    print(df.columns, "\n")

"""#### Preview DataSet"""

# Melakukan iterasi untuk setiap pasangan nama dataset dan DataFrame-nya dalam dictionary datasets
for name, df in datasets.items():
    # Menampilkan judul preview nama dataset
    print(f"\nPreview {name}")

    # Menampilkan 3 baris pertama dari dataset sebagai preview
    display(df.head(3))

"""## **3. Exploratory Data Analysis**

Exploratory Data Analysis (EDA) adalah tahap penting dalam proses analisis data yang bertujuan untuk memahami karakteristik dan struktur data secara menyeluruh sebelum dilakukan pemodelan atau analisis lebih lanjut.

### Informasi dataset
Fungsi Ringkas untuk Menampilkan Informasi dan Preview DataFrame
"""

# Fungsi ringkas info + head
def show_data_info(df, name):
    print(f"--- Info {name} ---")
    print(df.info())
    print(df.head())
    print("\n")

# Panggil fungsi dengan nama variabel DataFrame yang benar
show_data_info(anime_df, 'anime.csv')
show_data_info(animelist_df, 'animelist.csv')
show_data_info(recommendations_df, 'anime_recommendations.csv')
show_data_info(rating_df, 'rating_complete.csv')
show_data_info(status_df, 'watching_status.csv')

"""#### Pemeriksaan Kolom dan Statistik Unik pada Dataset

"""

# Print the columns of rating_df to confirm the column names
print(rating_df.columns)

# Assuming rating_df contains 'user_id' and 'anime_id'
print("Status unik:", status_df['status'].unique())
print("Jumlah status unik:", status_df['status'].nunique())

# Use rating_df to count unique users and anime
print("Jumlah user unik:", rating_df['user_id'].nunique())
print("Jumlah anime unik:", rating_df['anime_id'].nunique())

"""#### Missing Values"""

# melihat missing values di datasets

for name, df in datasets.items():
    print(f"\nMissing values in {name}:")
    print(df.isnull().sum())

"""Insight:

Dari hasil pemeriksaan missing values pada masing-masing dataset, diperoleh informasi sebagai berikut:

- anime.csv: Hampir semua kolom tidak memiliki nilai yang hilang, kecuali kolom `Synopsis` terdapat 6 missing values.
- animelist.csv, anime_recommendations.csv, dan rating_complete.csv: Tidak terdapat missing values sama sekali.
- watching_status.csv: Tidak ditemukan missing values, meskipun pada kolom `description` terdapat spasi awal yang perlu diperhatikan saat pemrosesan.

## Visualisasi

Visualisasi data adalah teknik penting dalam analisis data yang digunakan untuk menyajikan data secara grafis atau visual agar informasi yang terkandung dalam data menjadi lebih mudah dipahami dan dianalisis.
"""

# Visualisasi jumlah anime berdasarkan tipe (Type)
plt.figure(figsize=(8,5))
sns.countplot(data=anime_df, x='Type', order=anime_df['Type'].value_counts().index)
plt.title('Jumlah Anime berdasarkan Tipe')
plt.xlabel('Tipe')
plt.ylabel('Jumlah')
plt.xticks(rotation=45)
plt.show()

"""Insight:

Grafik batang ini menunjukkan jumlah anime yang dikategorikan berdasarkan tipe seperti TV, Movie, OVA, Web, Music, DVD, dan Other. Dari visualisasi terlihat bahwa:

* Tipe TV mendominasi dengan jumlah anime terbanyak, hampir mencapai 6000 judul.
* Movie menempati posisi kedua dengan sekitar 2900 anime.
* Tipe lain seperti OVA, Web, dan Music memiliki jumlah yang relatif seimbang, berkisar antara 1900–2200 judul.
* Tipe DVD dan Other memiliki jumlah yang lebih sedikit, kurang dari 1000 judul.
* Terdapat label TV di ujung kanan yang kemungkinan merupakan data duplikat atau kategori khusus dengan jumlah sangat kecil (104).

Visualisasi ini membantu memahami distribusi dan fokus produksi anime berdasarkan tipe yang berbeda.


"""

# Visualisasi 10 anime dengan rata-rata rating tertinggi berdasarkan data rating pengguna
avg_ratings = rating_df.groupby('anime_id')['rating'].mean().sort_values(ascending=False).head(10)

# Filter anime_df untuk mendapatkan informasi judul anime dari anime_id teratas
top_anime_info = anime_df[anime_df['Anime-PlanetID'].isin(avg_ratings.index)]

# Pastikan urutan judul anime sesuai dengan urutan rating tertinggi
anime_names = top_anime_info.set_index('Anime-PlanetID').loc[avg_ratings.index]['Name']

# Plot barplot horizontal dari 10 anime dengan rating tertinggi
plt.figure(figsize=(10,6))
sns.barplot(x=avg_ratings.values, y=anime_names.values)
plt.title('10 Anime dengan Rata-rata Rating Tertinggi')
plt.xlabel('Rata-rata Rating')
plt.ylabel('Judul Anime')
plt.show()

"""Insigt:

Grafik menunjukkan 10 anime dengan rata-rata rating tertinggi. Against the Gods menempati posisi pertama, diikuti oleh Fullmetal Alchemist: Brotherhood dan Gintama (2015). Judul seperti Attack on Titan, Hunter x Hunter, dan Jujutsu Kaisen juga masuk daftar. Beberapa judul Gintama muncul lebih dari sekali, menandakan konsistensi kualitas.

"""

# Print the column names of the recommendations_df to identify the correct column name
print(recommendations_df.columns)

# Assuming the column related to the anime being recommended is named 'Anime' based on the truncated name:
top_reco = recommendations_df['Anime'].value_counts().head(10)
anime_names = anime_df.set_index('Anime-PlanetID').loc[top_reco.index]['Name']

plt.figure(figsize=(10,6))
sns.barplot(x=top_reco.values, y=anime_names.values)
plt.title('Anime dengan Rekomendasi Terbanyak')
plt.xlabel('Jumlah Direkomendasikan'); plt.ylabel('Anime'); plt.show()

"""Insight:

Grafik menunjukkan 10 anime dengan jumlah rekomendasi terbanyak. Death Note berada di posisi pertama, diikuti oleh Neon Genesis Evangelion dan Ouran High School Host Club. Semua judul dalam daftar ini merupakan anime populer yang sering disarankan kepada penonton lain.

"""

status_map = {
    1: 'Watching',
    2: 'Completed',
    3: 'On-Hold',
    4: 'Dropped',
    6: 'Plan to Watch'
}

# Mengganti watching_status dengan nama variabel DataFrame yang benar yaitu status_df
status_counts = status_df['status'].map(status_map).value_counts()
status_map = {
    1: 'Watching',
    2: 'Completed',
    3: 'On-Hold',
    4: 'Dropped',
    6: 'Plan to Watch'
}

# Mengganti watching_status dengan nama variabel DataFrame yang benar yaitu status_df
status_counts = status_df['status'].map(status_map).value_counts()

plt.figure(figsize=(6,6))
plt.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Proporsi Status Menonton')
plt.axis('equal')
plt.show()
plt.show()

"""Insight:

Diagram pie menunjukkan proporsi status menonton anime yang terbagi rata. Setiap kategori: Watching, Completed, On-Hold, Dropped, dan Plan to Watch, memiliki proporsi yang sama, yaitu 20%.

## **4. Data Preparation**

Data preparation (persiapan data) adalah tahap penting dalam proses analisis data atau pembangunan model machine learning. Tujuannya adalah memastikan bahwa data bersih, konsisten, dan relevan untuk dianalisis atau digunakan dalam model.

#### Penggabungan data
"""

# Pilih kolom-kolom tertentu dari dataset anime
selected_columns = ['Anime-PlanetID', 'Name', 'Rating Score', 'Tags']
anime_subset = anime_df[selected_columns]

anime_subset.info()

# Ekstrak kolom userID, animeID, dan status dari animelist_df
animelist_data = animelist_df.iloc[:,:3]

animelist_data.info()

# Menggabungkan data fitur anime dengan data daftar tontonan berdasarkan ID, lalu rapikan urutan kolom
merged_anime = pd.merge(anime_subset, animelist_data, left_on='Anime-PlanetID', right_on='anime_id')
merged_anime = merged_anime.drop(columns='Anime-PlanetID')

# Memindahkan kolom anime_id ke posisi paling depan untuk kejelasan struktur
anime_id_col = merged_anime.pop('anime_id')
merged_anime.insert(0, 'anime_id', anime_id_col)

merged_anime.head()

merged_anime.info()

"""#### Memeriksa Missing Values"""

# cek missing value
merged_anime.isna().sum()

"""Insight:

Tidak terdapat missing values pada merged_anime.
"""

# copy nilai ke variabel baru
anime_selected = merged_anime.copy()
anime_selected.head()

"""#### Text Cleaning untuk dataset anime_selected pada variabel Anime
Tujuan:
- Membersihkan teks dari simbol HTML (seperti &quot;, &#039;, &amp;) agar mudah dibaca dan diproses.
- Menangani kasus khusus seperti .hack// dan 'A's' yang bisa mengganggu hasil analisis jika tidak dihapus atau diubah.
"""

# bersihkan teks
def text_cleaning(text):
    text = re.sub(r'&quot;', '', text)
    text = re.sub(r'.hack//', '', text)
    text = re.sub(r'&#039;', '', text)
    text = re.sub(r'A&#039;s', '', text)
    text = re.sub(r'I&#039;', 'I\'', text)
    text = re.sub(r'&amp;', 'and', text)

    return text

# Terapkan pembersihan teks
anime_selected['Name'] = anime_selected['Name'].apply(text_cleaning)

anime_selected

"""#### Memeriksa missing values anime_selected"""

# cek missing value
anime_selected.isnull().sum()

"""Insight:
Tidak terdapat missing values pada anime_selected
"""

# Duplikasi anime_selected lalu susun baris berdasarkan kolom anime_id secara ascending
prepped_df = anime_selected.iloc[anime_selected['anime_id'].argsort()]
prepped_df.head()

# Duplikasi anime_selected lalu susun baris berdasarkan kolom anime_id secara ascending
prepped_df = prepped_df.drop_duplicates('anime_id')
prepped_df.head()

"""### Content Based Filtering

#### Menghapus tanda koma (,) dari kolom Tags dan menggantinya dengan spasi kosong (' ').
"""

tags_updated = prepped_df['Tags'].apply(lambda x: x.replace(',', ' '))
tags_updated

"""### Collaborative Filtering

#### Menyeleksi fitur yang digunakan untuk collaborative filtering
"""

# seleksi fitur
cbf_df = prepped_df.drop(['Rating Score', 'Tags'], axis=1)
cbf_df

"""Insight:

Untuk menyisakan hanya fitur yang dibutuhkan dalam Content-Based Filtering, misalnya Anime-PlanetID dan Name. Kolom yang dihapus dianggap tidak relevan pada tahap ini.

"""

# Filter pengguna yang telah memberi rating pada minimal 10 anime
user_rating_counts = cbf_df['user_id'].value_counts()
rating_df = cbf_df[cbf_df['user_id'].isin(user_rating_counts[user_rating_counts >= 10].index)].copy()
len(rating_df)

"""#### Insight

- Kode ini menghitung jumlah rating yang diberikan tiap pengguna (user_id) pada dataset cbf_df.

- Kemudian, hanya pengguna yang memberi rating sebanyak minimal 10 anime yang dipilih.

- Data hasil filter (rating_df) hanya berisi interaksi dari pengguna yang cukup aktif, yang bisa meningkatkan kualitas analisis atau model rekomendasi karena data pengguna yang lebih representatif.

- Penggunaan .copy() mencegah peringatan SettingWithCopyWarning di pandas saat memodifikasi data selanjutnya.

"""

# Konversi rating ke label biner (suka/tidak suka)
rating_df['label'] = rating_df['rating'].apply(lambda x: 1 if x >= 3.5 else 0)

# Ambil nilai minimum dan maksimum dari rating asli
min_val = rating_df['rating'].min()
max_val = rating_df['rating'].max()

# Normalisasi skor rating ke rentang 0–1
rating_df['rating'] = rating_df['rating'].apply(lambda x: (x - min_val) / (max_val - min_val)).astype(np.float64)

# (Opsional) Cetak rata-rata hasil normalisasi
average_rating = rating_df['rating'].mean()
print('Average Rating:', average_rating)

"""#### Insight

- Label biner dibuat berdasarkan threshold 3.5, yang memisahkan rating menjadi “suka” (1) dan “tidak suka” (0). Ini berguna untuk model klasifikasi atau rekomendasi berbasis preferensi sederhana.

- Skor rating asli kemudian dinormalisasi ke rentang 0 sampai 1. Normalisasi membantu dalam algoritma yang sensitif terhadap skala data, memastikan konsistensi dan mempercepat konvergensi.

- Penggunaan nilai min dan max dari rating asli menjamin transformasi linier yang sesuai dengan distribusi data.

- Menghitung dan mencetak rata-rata rating yang sudah dinormalisasi dapat memberikan gambaran umum tentang sentimen rata-rata dalam dataset setelah normalisasi.

- Penggunaan .astype(np.float64) memastikan tipe data konsisten untuk operasi numerik lanjutan.
"""

# Cek apakah ada data duplikat dalam rating_df dan hapus jika ditemukan
duplicate_flags = rating_df.duplicated()

if duplicate_flags.any():
    print(f'> {duplicate_flags.sum()} duplicate entries found')
    rating_df = rating_df.loc[~duplicate_flags]

print(f'> {rating_df.duplicated().sum()} duplicates remaining')

"""#### Insight

- rating_df.duplicated() mengidentifikasi baris duplikat berdasarkan seluruh kolom secara default.

- Jika ada duplikat, jumlahnya dicetak dan kemudian baris duplikat dihapus dengan memilih baris yang bukan duplikat (~duplicate_flags).

- Setelah penghapusan, kode memeriksa kembali untuk memastikan tidak ada duplikat yang tersisa, yang sangat penting untuk menjaga kualitas data.

  Langkah ini penting untuk menghindari bias atau perhitungan ganda yang bisa merusak analisis atau model rekomendasi.
"""

# Membuat daftar unik user_id tanpa duplikasi
unique_users = rating_df['user_id'].unique().tolist()

# Membuat kamus pemetaan dari user_id ke angka
user_to_idx = {user: idx for idx, user in enumerate(unique_users)}

# Membuat kamus balik dari angka ke user_id
idx_to_user = {idx: user for idx, user in enumerate(unique_users)}

# Menambahkan kolom baru 'user' dengan encoding numerik user_id
rating_df['user'] = rating_df['user_id'].map(user_to_idx)
total_users = len(user_to_idx)

# Membuat daftar unik anime_id
unique_animes = rating_df['anime_id'].unique().tolist()

# Membuat kamus pemetaan dari anime_id ke angka
anime_to_idx = {anime: idx for idx, anime in enumerate(unique_animes)}

# Membuat kamus balik dari angka ke anime_id
idx_to_anime = {idx: anime for idx, anime in enumerate(unique_animes)}

# Menambahkan kolom baru 'anime' dengan encoding numerik anime_id
rating_df['anime'] = rating_df['anime_id'].map(anime_to_idx)
total_animes = len(anime_to_idx)

print(f"Jumlah pengguna: {total_users}, Jumlah anime: {total_animes}")
print(f"Rating minimal: {rating_df['rating'].min()}, Rating maksimal: {rating_df['rating'].max()}")

"""#### Insight

Hasil ini mengindikasikan beberapa hal penting terkait dataset rating anime yang sudah diproses:

* Jumlah pengguna: 179
  Setelah proses filter dan pembersihan, data hanya mencakup 179 pengguna yang aktif memberi rating minimal 10 anime. Ini menunjukkan dataset fokus pada pengguna dengan interaksi yang cukup signifikan, yang biasanya menghasilkan data lebih berkualitas untuk analisis atau model rekomendasi.

* Jumlah anime: 8133
  Dataset berisi rating untuk 8.133 anime unik, yang menunjukkan cakupan anime cukup luas. Hal ini memberikan keragaman pilihan dan tantangan dalam membangun sistem rekomendasi yang mampu memahami preferensi pengguna di berbagai anime.

* Rating minimal: 0.0, Rating maksimal: 1.0
  Rating telah dinormalisasi ke rentang 0 sampai 1, dengan 0 mewakili rating terendah (aslinya nilai minimum sebelum normalisasi) dan 1 mewakili rating tertinggi. Normalisasi ini penting agar data siap untuk digunakan dalam model yang sensitif terhadap skala nilai, serta memudahkan perbandingan antar rating.

"""

# Acak data rating_df dengan seed 39 agar hasil konsisten
shuffled_df = rating_df.sample(frac=1, random_state=39)

# Ambil fitur 'user' dan 'anime' sebagai input model
X = shuffled_df[['user', 'anime']].to_numpy()

# Ambil nilai rating sebagai target output
y = shuffled_df['rating'].to_numpy()

# Tentukan batas indeks untuk pembagian data train (80%) dan test (20%)
split_idx = int(0.8 * len(shuffled_df))

# Pisahkan data menjadi training dan testing berdasarkan indeks
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print(f'> Jumlah rating di train set: {len(y_train)}')
print(f'> Jumlah rating di test set: {len(y_test)}')

"""#### Insight

- Data diacak secara acak menggunakan random_state=39 untuk memastikan hasil pengacakan bisa direproduksi setiap kali dijalankan.

- Fitur input (X) hanya menggunakan encoding numerik dari user dan anime, yang biasanya menjadi input utama dalam sistem rekomendasi berbasis interaksi.

- Target (y) adalah rating yang sudah dinormalisasi, digunakan sebagai output yang ingin diprediksi oleh model.

- Data dibagi menjadi 80% untuk pelatihan dan 20% untuk pengujian, proporsi standar yang memberikan keseimbangan antara pelatihan yang cukup dan evaluasi yang valid.

- Jumlah data cukup besar, dengan 6.506 sampel untuk training dan 1.627 sampel untuk testing, memastikan model dapat belajar dari data yang beragam dan diuji pada data yang tidak terlihat sebelumnya.

## **5. Modelling**

### **Content Based Filtering**

#### Inisialisasi dan penerapan TF-IDF Vectorizer pada data tags
"""

# Inisialisasi TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity # Import cosine_similarity

tf = TfidfVectorizer()


tfidf_matrix = tf.fit_transform(tags_updated)

# Mapping array dari fitur index integer ke fitur nama
tfidf_matrix.shape

"""#### Insight

Model TF-IDF berhasil mengubah data teks dari 16.621 anime ke dalam ruang fitur berdimensi 616, yang memungkinkan perhitungan kesamaan (misalnya cosine similarity) antara anime berdasarkan tag atau deskripsi mereka. Dimensi fitur yang cukup besar ini memberikan detail yang cukup untuk membedakan konten antar anime, tapi tetap cukup efisien untuk proses komputasi.
"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=prepped_df['Name']
).sample(50, axis=1).sample(10, axis=0)

"""#### Insight

sangat besar supaya bisa diperiksa atau divisualisasi dengan mudah, dapat melihat 10 anime secara acak dan hanya 50 kata (fitur) secara acak. Ini sangat membantu untuk eksplorasi data tanpa harus memproses seluruh matriks besar sekaligus.

"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=prepped_df['Name'], columns=prepped_df['Name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap anime
cosine_sim_df

"""#### Insight

Matriks cosine similarity berukuran (16621, 16621) berarti kemiripan dihitung untuk setiap pasangan dari 16.621 anime, menghasilkan matriks besar yang menunjukkan seberapa mirip tiap anime satu sama lain.

"""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df, items=prepped_df[['Name', 'Tags']], k=5):
        index = similarity_data.loc[:,nama_anime].to_numpy().argpartition(
            range(-1, -k, -1))

        # Mengambil data dengan similarity terbesar dari index yang ada
        closest = similarity_data.columns[index[-1:-(k+2):-1]]

        # Drop nama_anime agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
        closest = closest.drop(nama_anime, errors='ignore')

        return pd.DataFrame(closest).merge(items).head(k)

"""#### Insight:

Fungsi ini implementasi sederhana dan efisien untuk sistem rekomendasi berbasis konten yang mencari anime mirip berdasarkan fitur teks (tags). Cocok digunakan untuk memberikan saran anime serupa.
"""

# mendapatkan nama anime random
anime_title = np.random.choice(prepped_df.Name)
anime_title
prepped_df[prepped_df.Name.eq(anime_title)]

"""#### Insight

- Ini berguna untuk eksplorasi data secara acak, misalnya melihat contoh anime tertentu beserta detailnya seperti anime_id, Rating Score, Tags, dan data rating pengguna.

- Dari contoh output, anime "Steel Fire Brigade Fire Robo" memiliki rating normalisasi 0.0, artinya rating aslinya adalah rating terendah di dataset.

- Pendekatan ini dapat digunakan untuk sampling anime saat pengujian fungsi rekomendasi atau analisis.
"""

anime_recommendations(anime_title)

"""#### Insight

Rekomendasi yang dihasilkan menunjukkan anime yang memiliki kesamaan tag dan tema dengan anime acak yang dipilih, "Steel Fire Brigade Fire Robo"

Dari daftar:

* Mayoritas anime berhubungan dengan genre Action, Mecha, Sci Fi, Family Friendly, Korean, yang mengindikasikan sistem berhasil menangkap kesamaan konten berdasarkan tag.
* Anime rekomendasi juga berfokus pada jenis konten pendek dan promosi, sesuai dengan anime input yang juga berkonsep serupa.

Ini menandakan fungsi rekomendasi berjalan efektif, memberikan pilihan anime yang relevan dan sesuai preferensi konten dari anime sumbernya.

### **Collaborative Based Filtering**
"""

# Import libraries
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Dot, Dense, Flatten, Activation, BatchNormalization
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np # Make sure numpy is imported

# Dapatkan jumlah user dan anime unik
# Ensure n_users and n_animes are calculated based on the encoded values
n_users = rating_df['user'].nunique() # Use the encoded user column
n_animes = rating_df['anime'].nunique() # Use the encoded anime column

# Persiapan data input dan label using the encoded columns
X = rating_df[['user', 'anime']].values # Use the encoded 'user' and 'anime' columns
y = rating_df['label'].values

# Split data training dan testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pisahkan user dan anime for train and test (these are already encoded indices)
X_train_user = X_train[:, 0]
X_train_anime = X_train[:, 1]
X_test_user = X_test[:, 0]
X_test_anime = X_test[:, 1]

# Definisi model RecommenderNet
def RecommenderNet(num_users, num_anime, embedding_size):
    user_input = Input(name='user', shape=(1,))
    anime_input = Input(name='anime', shape=(1,))

    user_embedding = Embedding(
        name='user_embedding',
        input_dim=num_users,
        output_dim=embedding_size
    )(user_input)

    anime_embedding = Embedding(
        name='anime_embedding',
        input_dim=num_anime,
        output_dim=embedding_size
    )(anime_input)

    dot_product = Dot(name='dot_product', normalize=True, axes=2)([user_embedding, anime_embedding])
    x = Flatten()(dot_product)
    x = Dense(1, kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Activation("sigmoid")(x)

    model = Model(inputs=[user_input, anime_input], outputs=x)
    return model

# Inisialisasi model
model = RecommenderNet(n_users, n_animes, embedding_size=50)

# Compile model
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# (Opsional) Callbacks
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', patience=3, restore_best_weights=True
)

# Training model
history = model.fit(
    x=[X_train_user, X_train_anime],
    y=y_train,
    batch_size=64,
    epochs=2,
    validation_data=([X_test_user, X_test_anime], y_test),
    callbacks=[early_stopping]
)

"""#### Insight

* Data pengguna (`user`) dan anime (`anime`) sudah di-encode menjadi indeks numerik, memudahkan penggunaan embedding.
* Model `RecommenderNet` memakai dua embedding layer terpisah untuk pengguna dan anime, masing-masing berukuran `embedding_size` (50).
* Output embedding dihitung dot product dan dinormalisasi, lalu diproses oleh layer `Dense`, `BatchNormalization`, dan `sigmoid` activation untuk menghasilkan prediksi probabilitas suka/tidak suka (label biner).
* Model di-compile dengan loss binary crossentropy dan metric RMSE, cocok untuk masalah klasifikasi biner dengan evaluasi error prediksi.
* Training dilakukan dengan batch size 64 dan early stopping untuk menghindari overfitting.
* Dengan 2 epoch, ini merupakan langkah awal pelatihan — bisa ditingkatkan untuk hasil lebih baik.

Kesimpulan:
Kode ini mengimplementasikan model rekomendasi berbasis embedding yang umum digunakan untuk prediksi interaksi user-item (rating). Model ini memanfaatkan representasi latent dari pengguna dan anime, sehingga dapat menangkap pola preferensi secara efisien.

#### Insight

Hasil training ini menunjukkan beberapa hal penting:

* **Epoch 1:**

  * Loss turun dari 0.7418 di training menjadi 0.6459 di validation,RMSE 0.5196 dan RMSE validation 0.4758
  * Ini menandakan model mulai belajar pola dari data dan belum overfitting.

* **Epoch 2:**

  * Loss training menurun lagi menjadi 0.6412, namun loss validation justru naik ke 0.6117, RMSE 0.4746 dan RMSE validation naik ke 0.4576
  * Ini bisa mengindikasikan model mulai sedikit overfitting atau fluktuasi pada validasi, mungkin karena epoch yang terlalu sedikit atau data yang kompleks.
"""

# Membersihkan data dengan mengganti nilai "Unknown" menjadi NaN pada dataframe anime_df

df_test = anime_df
df_test = df_test.replace("Unknown", np.nan)
df_test

"""#### Insight

- Kode ini mengganti semua nilai string "Unknown" di anime_df menjadi np.nan (nilai kosong/missing) agar lebih mudah ditangani dalam analisis atau pemrosesan data selanjutnya.

- Ini langkah umum dalam preprocessing data untuk menghindari masalah saat operasi numerik atau analisis statistik.
"""

# Mengganti nama kolom 'Anime-PlanetID' menjadi 'anime_id' untuk konsistensi
df_test.rename(columns = {'Anime-PlanetID':'anime_id'}, inplace = True)

"""#### Insight

Rename kolom ini dilakukan agar nama kolom konsisten dengan penamaan yang digunakan di bagian lain, misalnya anime_id yang sudah dipakai di dataset rating.

#### Fungsi rekomendasi anime untuk user tertentu menggunakan model prediksi rating
"""

def get_recommendations(user_id, recom_n=5):
    print("===" * 10)
    print("Recommendation for id: {}".format(user_id))
    print("===" * 10)

    animes_watched_by_user = rating_df[rating_df.user_id==user_id]
    anime_not_watched_df = df_test[
        ~df_test["anime_id"].isin(animes_watched_by_user.anime_id.values)
    ]

    # Check if anime2anime_encoded exists before using it
    if 'anime2anime_encoded' not in globals():
        print("Error: anime2anime_encoded mapping not found.")
        return pd.DataFrame() # Return empty DataFrame or handle as appropriate

    anime_not_watched_ids = list(
        set(anime_not_watched_df['anime_id']).intersection(set(anime2anime_encoded.keys()))
    )


    anime_not_watched_encoded = [[anime2anime_encoded.get(x)] for x in anime_not_watched_ids]

    # Check if user2user_encoded exists before using it
    if 'user2user_encoded' not in globals():
        print("Error: user2user_encoded mapping not found.")
        return pd.DataFrame() # Return empty DataFrame or handle as appropriate


    user_encoder = user2user_encoded.get(user_id)

    # Handle case where user_id is not found in mapping
    if user_encoder is None:
         print(f"Error: User ID {user_id} not found in user mapping.")
         return pd.DataFrame()

    if not anime_not_watched_encoded: # Check if there are any anime left to recommend
        print("No anime left to recommend for this user.")
        return pd.DataFrame()

    user_anime_array = np.hstack(
        ([[user_encoder]] * len(anime_not_watched_encoded), anime_not_watched_encoded)
    )

    user_anime_array = [user_anime_array[:, 0], user_anime_array[:, 1]]
    ratings = model.predict(user_anime_array).flatten()

    top_ratings_indices = (-ratings).argsort()[:recom_n]

    # Check if anime_encoded2anime exists before using it
    if 'anime_encoded2anime' not in globals():
        print("Error: anime_encoded2anime mapping not found.")
        return pd.DataFrame() # Return empty DataFrame or handle as appropriate

    recommended_anime_ids = [
        anime_encoded2anime.get(anime_not_watched_encoded[x][0]) for x in top_ratings_indices
    ]

    Results = []

    for anime_id in recommended_anime_ids: # Iterate directly over recommended_anime_ids
        try:
            condition = (df_test.anime_id == anime_id)
            # Use .iloc[0] to safely access the first row of the filtered DataFrame
            name = df_test[condition]['Name'].iloc[0] # Corrected column name to 'Name' (from previous prints)
            sypnopsis = df_test[condition]['Synopsis'].iloc[0] # Corrected column name to 'Synopsis' (from previous prints)
            tags = df_test[condition]['Tags'].iloc[0] # Corrected column name to 'Tags' (from previous prints)
        except IndexError:
            # This can happen if recommended_anime_id doesn't exist in df_test
            continue
        except KeyError as e:
             print(f"Error accessing column: {e}") # Print specific KeyError for debugging
             continue


        Results.append({#"anime_id": id_,
                        "name": name,
                        "pred_rating": ratings[recommended_anime_ids.index(anime_id)], # Get rating for this specific anime_id
                        "tags": tags,
                        'sypnopsis': sypnopsis} )


    print('>>>>>>>> Top ',recom_n,' anime recommendations for you <<<<<<<<<<<<')


    Results = pd.DataFrame(Results).sort_values(by='pred_rating', ascending=False)
    return Results

"""#### Insight

* Fungsi menerima `user_id` dan jumlah rekomendasi `recom_n`.
* Mencari anime yang belum ditonton user berdasarkan `rating_df` dan `df_test`.
* Menggunakan mapping encoding `user2user_encoded` dan `anime2anime_encoded` untuk mengubah ID asli ke indeks embedding.
* Membuat input array pasangan (user, anime) untuk prediksi model rekomendasi.
* Memanggil `model.predict` untuk mendapatkan prediksi rating (probabilitas suka) pada anime yang belum ditonton.
* Mengurutkan anime berdasarkan prediksi rating tertinggi dan mengambil top-N.
* Mengembalikan DataFrame berisi nama, prediksi rating, tag, dan sinopsis anime rekomendasi.
* Ada beberapa pengecekan error penting untuk memastikan mapping dan data lengkap, sehingga meminimalisir error runtime.
* Jika tidak ada anime tersisa atau user tidak ditemukan, fungsi memberi pesan dan mengembalikan DataFrame kosong.

### **Collaborative Based Filtering**
"""

# Memilih satu user secara acak yang memiliki jumlah rating kurang dari 500
ratings_per_user = rating_df.groupby('user_id').size()
random_user = ratings_per_user[ratings_per_user < 500].sample(1, random_state=None).index[0]
print('> user_id:', random_user)

"""#### Insight

* `ratings_per_user` menghitung jumlah rating yang diberikan tiap user.
* Dengan membatasi `ratings_per_user < 500`, kode memilih user dengan aktivitas rating yang tidak terlalu tinggi, sehingga fokus pada pengguna biasa/bukan super user.
* `sample(1)` mengambil satu user acak dari kelompok tersebut.
* Ini berguna untuk testing rekomendasi pada user dengan data interaksi terbatas, merepresentasikan kasus umum.
* Output `user_id: 72703` adalah contoh user yang dipilih secara acak sesuai kriteria.

"""

# Menyiapkan mapping antara ID asli dan encoding numerik (dan sebaliknya) untuk anime dan user

anime2anime_encoded = anime_to_idx  # dari bagian data preparation
anime_encoded2anime = idx_to_anime

user2user_encoded = user_to_idx
user_encoded2user = idx_to_user

"""#### Insight

Variabel anime2anime_encoded dan user2user_encoded menyimpan mapping dari ID asli (anime_id, user_id) ke indeks numerik yang digunakan di embedding model.

anime_encoded2anime dan user_encoded2user adalah mapping balik dari indeks numerik ke ID asli, berguna untuk interpretasi hasil rekomendasi.
"""

get_recommendations(random_user)

# Generate recommendation
top_10_df = get_recommendations(user_id=random_user, recom_n=10)

# Visualize
top_n = top_10_df.sort_values(by='pred_rating', ascending=True)

plt.figure(figsize=(10, 6))
plt.barh(top_n['name'], top_n['pred_rating'], color='skyblue')
plt.xlabel("Predicted Score")
plt.title(f"Top 10 Anime Recommendations for User {random_user}")
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""#### Insight

Berikut beberapa insight dari data anime yang kamu berikan:

1. **Peringkat Prediksi (pred\_rating):**

   * Nilai prediksi rating tertinggi adalah untuk *Katana Maidens: Toji no Miko Recap* (0.470138), dan nilai terendah pada daftar ini adalah *Servamp: Hai ni Mamireta Shoukei* (0.454850).
   * Perbedaan nilai prediksi antar anime tidak terlalu besar (semua di kisaran 0.45-0.47), yang mungkin menunjukkan model prediksi memberikan skor cukup dekat untuk anime-anime ini.

2. **Genre dan Tags:**

   * Anime dengan prediksi rating tertinggi (*Katana Maidens*) punya genre beragam seperti Action, School Life, Supernatural, Sword Fighting, dan Recap. Genre yang kompleks dan kaya bisa menarik banyak penonton.
   * Ada anime dengan genre yang lebih niche atau spesifik seperti *Pinocchio-P: E no Umakatta Tomodachi* (Art, Vocaloid) dan *Shijin no Shougai* (Abstract, Shorts).
   * Genre seperti Shorts dan Original Work ada di dua anime dengan rating prediksi yang lebih rendah, mungkin karena durasi pendek atau kurangnya eksposur.

3. **Sinopsis:**

   * Beberapa anime belum memiliki sinopsis (Pinocchio-P, Yakusoku, Servamp), kemungkinan ini memengaruhi daya tarik pengguna karena kurang informasi.
   * Anime dengan sinopsis lebih jelas (*Katana Maidens* dan *Shijin no Shougai*) cenderung memiliki prediksi rating sedikit lebih tinggi, walaupun tidak mutlak.

## **6. Evaluation**

Evaluasi pada sistem rekomendasi seperti grafik yang kamu tampilkan berfungsi untuk mengukur seberapa baik model memberikan rekomendasi yang relevan dan bermanfaat bagi pengguna.

### Content Based Filtering
"""

# Implementasi fungsi metrik Precision@K (hanya precision)
def calculate_precision_at_k(recommended_items, relevant_items, k):
    """
    Menghitung Precision@K.

    Args:
        recommended_items (list): Daftar item yang direkomendasikan.
        relevant_items (list): Daftar item yang relevan (ground truth).
        k (int): Jumlah item teratas yang dievaluasi.

    Returns:
        float: Precision@K
    """
    # Ambil hanya top-K item yang direkomendasikan
    top_k_recommended = recommended_items[:k]

    # Hitung jumlah item yang relevan dalam top-K rekomendasi
    relevant_in_top_k = len(set(top_k_recommended) & set(relevant_items))

    # Hitung Precision@K
    precision = relevant_in_top_k / k if k > 0 else 0

    return precision

# Pilih beberapa anime untuk dievaluasi
# Pastikan nama-nama ini ada di DataFrame prepped_df['Name']
anime_to_evaluate = ['Naruto', 'One Piece', 'Attack on Titan'] # Ganti dengan nama anime yang ada di prepped_df

# Tentukan K untuk evaluasi
k_value = 10

print(f"=== Evaluasi Content-Based Filtering (Simulasi) untuk K = {k_value} (Precision Only) ===")

for anime_name in anime_to_evaluate:
    # Dapatkan rekomendasi dari model Content-Based Filtering
    # Pastikan fungsi anime_recommendations sudah didefinisikan sebelumnya
    recommended_df = anime_recommendations(anime_name, k=k_value)
    recommended_list = recommended_df['Name'].tolist()

    # === Bagian simulasi ground truth - sesuaikan dengan data Anda ===
    # Untuk contoh ini, kita mensimulasikan item relevan berdasarkan tag serupa.
    # Dalam skenario nyata, ini akan berasal dari data ground truth yang sebenarnya.
    relevant_items = []
    try:
        # Temukan tag untuk anime target
        target_tags = prepped_df[prepped_df['Name'] == anime_name]['Tags'].iloc[0].split(' ')
        # Temukan anime lain dengan setidaknya satu tag yang sama
        relevant_anime_for_target_df = prepped_df[
            prepped_df['Tags'].apply(lambda x: any(tag in x for tag in target_tags))
        ]
        relevant_items = relevant_anime_for_target_df['Name'].tolist()
        # Hapus anime target itu sendiri dari daftar relevan
        if anime_name in relevant_items:
            relevant_items.remove(anime_name)

    except IndexError:
        print(f"Warning: Anime '{anime_name}' not found in prepped_df or has no tags.")
        continue # Skip evaluation if anime not found or no tags

    # ===============================================================

    # Hitung Precision@K hanya jika ada item relevan yang disimulasikan
    if len(relevant_items) > 0:
        precision = calculate_precision_at_k(recommended_list, relevant_items, k_value)

        print(f"\nAnime: {anime_name}")
        print(f"Precision@{k_value}: {precision:.4f}")
    else:
        print(f"\nAnime: {anime_name}")
        print("Tidak ada item relevan yang disimulasikan untuk evaluasi.")

"""#### Insight

1. Precision@10 = 1.0 Artinya Sangat Tinggi
    - Semua rekomendasi (10 dari 10) memiliki kemiripan tag dengan anime target.
    - Ini menunjukkan model Content-Based Filtering bekerja sangat baik dalam kesamaan konten/tag.

### Collaborative Based Filtering
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#### Insight

Model menunjukkan kinerja yang membaik selama proses pelatihan awal. Hal ini terlihat dari penurunan nilai Root Mean Squared Error (RMSE)pada kedua data:

* Train RMSE menurun dari 0.51 menjadi 0.475
* Test RMSE menurun dari 0.475 menjadi 0.457

Penurunan ini menunjukkan bahwa model berhasil mempelajari pola dari data pelatihan sambil tetap menjaga kemampuan generalisasi terhadap data pengujian. Karena kedua kurva menurun secara konsisten tanpa adanya gap besar, belum terlihat tanda-tanda overfitting. Ini berarti model tidak hanya menghafal data latih, tetapi juga mampu bekerja cukup baik pada data yang belum pernah dilihat.

Secara keseluruhan, ini adalah indikasi awal yang positif terhadap stabilitas dan potensi performa model.


"""

X_test_user = X_test[:, 0]
X_test_anime = X_test[:, 1]
test_dataset = tf.data.Dataset.from_tensor_slices(((X_test_user, X_test_anime), y_test))
test_dataset = test_dataset.batch(256).prefetch(tf.data.AUTOTUNE)
loss, accuracy = model.evaluate(test_dataset)

"""#### Insight

- Nilai RMSE ini relatif rendah, yang berarti bahwa rata-rata kesalahan prediksi model terhadap rating asli sekitar 0.45 poin.

- Dalam skala rating 1–10 (atau 0–10), ini cukup baik—model bisa memprediksi cukup dekat ke nilai aslinya.
"""